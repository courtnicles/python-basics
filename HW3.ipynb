{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (10%) What about the main inventions for GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           |                                                                                                                                                                                                                                                                                                            |\n",
    "|:-----------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| GoogLeNet | The network was much deeper than previous CNNs. <br>This was made possible by subnetworks called inception modules,<br>which allow GoogLeNet to useparameters much more efficiently than previous architectures                                                                                            |\n",
    "| ResNet    | It introduces the concept of layer skipping which allow<br>the gradient to directly go to the linked layer allowing very deep network without<br>gradient vanishing                                                                                                                                        |\n",
    "| SENet     | SE block. An SE block analyzes the output of the unit it is attached to, <br>focusing exclusively onthe depth dimension<br>For example, an SE block may learn that<br>mouths, noses, and eyes usually appear together in pictures: if you see a mouth and a<br>nose, you should expect to see eyes as well |\n",
    "| Xception  | A variant of GoogLeNet. It replaces the inception modules with a special type of layer<br>called a depthwise separable convolution layer. Separable convolutional layer<br>makes the strong assumption that spatial patterns and cross-channel patterns can be<br>modeled separately.                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (10%) Can you compare the difference between the word-level embedding and character-level embedding from text classification perspective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since character n grams are shared across words, these models do better than word embedding models for out of vocabulary words - they can generate an embedding for an OOV word. Word embedding models like word2vec cannot since they treat a word atomically.\n",
    "- Character embeddding models tend to do better than word embedding models for words that occur infrequently since the character n grams that are shared across words can still learn good embeddings. Word embedding models in contrast suffer from lack of enough training opportunity for infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (10%) BERT, proposed by Google, is another popular word embedding technique. Can you explain this and compare this with Word2Vec (Skip-gram, CBOW)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- “BERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.”\n",
    "- Let's look at this sentece: I eat a banana at Banana Republic store.\n",
    "    - For Word2Vec, the embedding of the two banana words would be the same. The connection between words such as POS or root word would come to play later in the case of GloVe.\n",
    "    - For BERT, instead just look at individual word vs the dictionary. We are provided with the context-dependent embedding. Therefore the two banana words will have different embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (25%, coding assignment) Try to follow the code about style transfer (https://www.tensorflow.org/tutorials/generative/style_transfer) with DL, you can select any style image to transform content image of your wonderful face (Your face photo) into your DL face. For this assignment, beside turn in your implementation code. You also have to submit TA your DL face image (try to adjust your DL face size to fit US passport photo size) with your name. We will have a Beauty Pageant for our class based on your DL face!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (30%, coding assignment) Implement Word2Vec from scratch, you can reference this website(https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281). You can choose your own texts corpse for training Word2Vec embedding model. Then, apply your own word-embedding model to perform sentiment classification for IMDB dataset (https://keras.io/api/datasets/imdb/) with CNN architecture. Try to demonstrate the effects of hyperparameters, e.g., parameters about Word2Vec and CNN model, to the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
